{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ee216ac-cbc1-4576-a6c2-74e6ae2ff2c9",
   "metadata": {},
   "source": [
    "# 6.4: Resampling Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451fa372-7eb2-43e8-bbf5-429bc446f0a0",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "### Import Libraries \n",
    "\n",
    "We import our standard libraries and specific objects/libraries at the top level of our notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c234885",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ISLP\n",
    "\n",
    "!pip install numpy==1.24.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6edbc31d-be7c-4dd3-bb07-a9f681754433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our previous libraries and objects\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from ISLP import load_data\n",
    "from ISLP.models import (ModelSpec as MS,\n",
    "                         summarize,\n",
    "                         poly)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import new libraries and objects\n",
    "from functools import partial\n",
    "from sklearn.model_selection import \\\n",
    "     (cross_validate,\n",
    "      KFold,\n",
    "      ShuffleSplit)\n",
    "from sklearn.base import clone\n",
    "from ISLP.models import sklearn_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f01389-fd9e-4824-8601-44ce42f58f98",
   "metadata": {},
   "source": [
    "## The Validation Set Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e417696-f8c8-4ef5-8404-e6838a907d47",
   "metadata": {},
   "source": [
    "We explore the use of the validation set approach in order to estimate the test error rates that result from fitting various linear models on the `Auto` data set.\n",
    "\n",
    "We use the function `train_test_split()` to split the data into training and validation sets. As there are 392 observations, we split into two equal sets of size 196 using the argument `test_size=196`. We want to set a random seed (`random_state=0`) so that the random results can be reproduced again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0419abe2-c733-4124-b202-45f6a8bf953e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Auto = load_data('Auto')\n",
    "Auto_train, Auto_valid = train_test_split(Auto,\n",
    "                                         test_size=196, # split in two\n",
    "                                         random_state=0) # random seed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6667129-56af-4d42-9416-2b51e60e3f0c",
   "metadata": {},
   "source": [
    "Now we can fit a linear regression using only the observations corresponding to the training set `Auto_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c531b5b-803b-42ed-88f0-e20242e0ad85",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_mm = MS(['horsepower'])\n",
    "X_train = hp_mm.fit_transform(Auto_train)\n",
    "y_train = Auto_train['mpg']\n",
    "model = sm.OLS(y_train, X_train)\n",
    "results = model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af93fbe6-df41-4f9f-878a-8b58b38fd0a0",
   "metadata": {},
   "source": [
    "We now use the `predict()` method of results evaluated on the model matrix for this model created using the validation data set. We also calculate the validation MSE of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dda5e219-dcd0-4569-a6b3-02954d68d2fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.61661706966988"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid = hp_mm.transform(Auto_valid)\n",
    "y_valid = Auto_valid['mpg']\n",
    "valid_pred = results.predict(X_valid)\n",
    "np.mean((y_valid - valid_pred)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252d8d71-405e-4b5f-92ea-365e44b2a8c1",
   "metadata": {},
   "source": [
    "So our estimated test MSE for the linear regression is 23.62.\n",
    "\n",
    "**What would happen if we used a different set of observations for our training set? Repeat the above\n",
    "process using a different seed (i.e. set.seed(2)) and report the differences.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c467449e-0c6d-413d-8f2c-97dccfbc5e5d",
   "metadata": {},
   "source": [
    "## Leave-One-Out Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa9709e-556e-4c23-81c8-3b75dd28c9a7",
   "metadata": {},
   "source": [
    "The simplest way to cross-validate in Python is to use `sklearn`. The `ISLP` package provides a wrapper (`sklearn_sm()`), that allows us to easily use the cross-validation tools of `sklear`n with models fit by `statsmodels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22117be0-29ae-4ccb-9b93-c5c538e90528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.23151351792924"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp_model = sklearn_sm(sm.OLS,\n",
    "                      MS(['horsepower']))\n",
    "X, Y = Auto.drop(columns=['mpg']), Auto['mpg']\n",
    "cv_results = cross_validate(hp_model,\n",
    "                            X,\n",
    "                            Y,\n",
    "                            cv=Auto.shape[0])\n",
    "cv_err = np.mean(cv_results['test_score'])\n",
    "cv_err"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45839a95-4991-43d4-bc15-a529d0121252",
   "metadata": {},
   "source": [
    "The arguments to `cross_validate()` are: an object with the appropriate `fit()`, `predict()`, and `score()` methods, an array of features `X` and a response `Y`. We also included an additional argument `cv` to `cross_validate()`; specifying an integer \n",
    "$K$ results in $K$-fold cross-validation.\n",
    "\n",
    "We can repeat this procedure for increasingly complex polynomial fits. To automate the process, we again use a for loop which iteratively fits polynomial regressions of degree 1 to 5, computes the associated cross-validation error, and stores it in the $i$th element of the vector `cv_error`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "752e078f-f2a6-40c9-9597-e45a0698b8d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24.23151352, 19.24821312, 19.33498406, 19.42443033, 19.03323827])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_error = np.zeros(5)\n",
    "H = np.array(Auto['horsepower'])\n",
    "M = sklearn_sm(sm.OLS)\n",
    "for i, d in enumerate(range(1,6)):\n",
    "    X = np.power.outer(H, np.arange(d+1))\n",
    "    M_CV = cross_validate(M,\n",
    "                          X,\n",
    "                          Y,\n",
    "                          cv=Auto.shape[0])\n",
    "    cv_error[i] = np.mean(M_CV['test_score'])\n",
    "cv_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc754b70-ba8d-43ba-afed-3596d132cedf",
   "metadata": {},
   "source": [
    "**Write a for loop that fits the Auto data using polynomials up to degree 10. Compute the LOOCV\n",
    "test error rate for each of them. Which degree polynomial fits the data the best according to\n",
    "the test error rate and the bias-variance trade-off?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2386414e-7ecb-400c-8960-0a1a177f772e",
   "metadata": {},
   "source": [
    "## k-Fold Cross-Validation\n",
    "\n",
    "Here we use `KFold()` to partition the data into $K=10$ random groups. We use random_state to set a random seed and initialize a vector `cv_error` in which we will store the CV errors corresponding to the polynomial fits of degrees one to five."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16c32ccf-4e6d-412d-a89a-7f954cb731d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24.20766449, 19.18533142, 19.27626666, 19.47848403, 19.13720581])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_error = np.zeros(5)\n",
    "cv = KFold(n_splits=10,\n",
    "           shuffle=True,\n",
    "           random_state=0) # use same splits for each degree\n",
    "for i, d in enumerate(range(1,6)):\n",
    "    X = np.power.outer(H, np.arange(d+1))\n",
    "    M_CV = cross_validate(M,\n",
    "                          X,\n",
    "                          Y,\n",
    "                          cv=cv)\n",
    "    cv_error[i] = np.mean(M_CV['test_score'])\n",
    "cv_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45191e7f-a7ad-4e2b-9f43-11ec0793179c",
   "metadata": {},
   "source": [
    "**Repeat this process using K = 10 and report the results.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2be2e8-fc23-4137-8537-ec035ffdfa0f",
   "metadata": {},
   "source": [
    "## The Bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5bbf66-b518-430a-8def-51eae4edba7c",
   "metadata": {},
   "source": [
    "The bootstrap approach can be applied in almost all situations. While there are several implementations of the bootstrap in Python, its use for estimating standard error is simple enough that we write our own function below for the case when our data is stored in a dataframe. We will use the `Portfolio` data set in the `ISLP` package to illustrate.\n",
    "\n",
    "We will create a function `alpha_func()`, which takes as input a dataframe `D` assumed\n",
    "to have columns `X` and `Y`, as well as a\n",
    "vector `idx` indicating which observations should be used to\n",
    "estimate \n",
    "$\\alpha$. The function then outputs the estimate for $\\alpha$ based on\n",
    "the selected observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "07cd0632-1ae7-463e-8e0f-6504fb70f3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Portfolio = load_data('Portfolio')\n",
    "def alpha_func(D, idx):\n",
    "   cov_ = np.cov(D[['X','Y']].loc[idx], rowvar=False)\n",
    "   return ((cov_[1,1] - cov_[0,1]) /\n",
    "           (cov_[0,0]+cov_[1,1]-2*cov_[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5244b3fc-be11-4566-8dcf-603e0035e796",
   "metadata": {},
   "source": [
    "The following command estimates $\\alpha$ using all 100 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6e4973de-b821-4af8-8e0f-61befe6d86a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.57583207459283"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_func(Portfolio, range(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e226aa-b7cf-49cc-9b57-2b716e163457",
   "metadata": {},
   "source": [
    "Next we randomly select 100 observations from range(100), with replacement. This is equivalent to constructing a new bootstrap data set and recomputing $\\alpha$ based on the new data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "71bfd7c4-02b0-45ad-99f4-616ace0f044c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6074452469619004"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng = np.random.default_rng(0)\n",
    "alpha_func(Portfolio,\n",
    "           rng.choice(100,\n",
    "                      100,\n",
    "                      replace=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee315a5-d48e-4c57-b406-c2940c04e794",
   "metadata": {},
   "source": [
    "This process can be generalized to create a simple function `boot_SE()` for computing the bootstrap standard error for arbitrary functions that take only a data frame as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4c616715-c7e3-4230-a313-58ba81bdd69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boot_SE(func,\n",
    "            D,\n",
    "            n=None,\n",
    "            B=1000,\n",
    "            seed=0):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    first_, second_ = 0, 0\n",
    "    n = n or D.shape[0]\n",
    "    for _ in range(B):\n",
    "        idx = rng.choice(D.index,\n",
    "                         n,\n",
    "                         replace=True)\n",
    "        value = func(D, idx)\n",
    "        first_ += value\n",
    "        second_ += value**2\n",
    "    return np.sqrt(second_ / B - (first_ / B)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc49225f-368c-467a-90c8-be415fc10a46",
   "metadata": {},
   "source": [
    "Letâ€™s use our function to evaluate the accuracy of our estimate of $\\alpha$ using $B = 1,000$ bootstrap replications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5c693db4-e496-4c57-99db-f558c6213874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09118176521277699"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_SE = boot_SE(alpha_func,\n",
    "                   Portfolio,\n",
    "                   B=1000,\n",
    "                   seed=0)\n",
    "alpha_SE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ee815d-2369-45bb-862f-baddb290820d",
   "metadata": {},
   "source": [
    "The final output shows that the bootstrap estimate for ${\\rm SE}(\\hat{\\alpha})$ is $0.0912$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b759abb-913b-40bf-b11e-890681f1f9e4",
   "metadata": {},
   "source": [
    "The bootstrap approach can be used to assess the variability of the coefficient estimates and predictions from a statistical learning method. Here we use the bootstrap approach in order to assess the variability of the estimates for \n",
    " and \n",
    ", the intercept and slope terms for the linear regression model that uses horsepower to predict mpg in the Auto data set\n",
    "\n",
    "We start by writing a generic function `boot_OLS()` for bootstrapping a regression model that takes a formula to define the corresponding regression. We use the `clone()` function to make a copy of the formula that can be refit to the new dataframe. This means that any derived features such as those defined by `poly()` which will be re-fit on the resampled data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14434d6e-89ec-4133-8898-85d4ad811dfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[39.88064456, -0.1567849 ],\n",
       "       [38.73298691, -0.14699495],\n",
       "       [38.31734657, -0.14442683],\n",
       "       [39.91446826, -0.15782234],\n",
       "       [39.43349349, -0.15072702],\n",
       "       [40.36629857, -0.15912217],\n",
       "       [39.62334517, -0.15449117],\n",
       "       [39.0580588 , -0.14952908],\n",
       "       [38.66688437, -0.14521037],\n",
       "       [39.64280792, -0.15555698]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def boot_OLS(model_matrix, response, D, idx):\n",
    "    D_ = D.loc[idx]\n",
    "    Y_ = D_[response]\n",
    "    X_ = clone(model_matrix).fit_transform(D_)\n",
    "    return sm.OLS(Y_, X_).fit().params\n",
    "\n",
    "hp_func = partial(boot_OLS, MS(['horsepower']), 'mpg')\n",
    "\n",
    "# Use the hp_func() function to create bootstrap estimates for the intercept and slope \n",
    "rng = np.random.default_rng(0)\n",
    "np.array([hp_func(Auto,\n",
    "          rng.choice(392,\n",
    "                     392,\n",
    "                     replace=True)) for _ in range(10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94384a9f-5e10-4854-9f89-1cce12b51731",
   "metadata": {},
   "source": [
    "Next, we use the `boot_SE()` {} function to compute the standard errors of 1,000 bootstrap estimates for the intercept and slope terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "483dee5f-e600-482f-b41f-b324570225d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "intercept     0.848807\n",
       "horsepower    0.007352\n",
       "dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp_se = boot_SE(hp_func,\n",
    "                Auto,\n",
    "                B=1000,\n",
    "                seed=10)\n",
    "hp_se"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5748d1ef-eb30-4538-85b3-78c619eb4d71",
   "metadata": {},
   "source": [
    "The bootstrap estimate for ${\\rm SE}(\\hat{\\beta}_0)$ is\n",
    "0.85, and that the bootstrap\n",
    "estimate for ${\\rm SE}(\\hat{\\beta}_1)$ is\n",
    "0.0074. Standard formulas can be used to compute the standard errors for the regression coefficients in a linear model. These can be obtained using the `summarize()` function from `ISLP.sm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f46fca2a-d45e-47d8-ac66-5aa0b110f16d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "intercept     0.717\n",
       "horsepower    0.006\n",
       "Name: std err, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp_model.fit(Auto, Auto['mpg'])\n",
    "model_se = summarize(hp_model.results_)['std err']\n",
    "model_se"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80895997-b92f-46e0-a329-d9bff98d8957",
   "metadata": {},
   "source": [
    "The standard error estimates for $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ are 0.717 for the intercept and 0.006 for the slope.\n",
    "\n",
    "Now, we want to compute the boostrap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "efde4625-8e96-404b-a4e2-89ecf2f6faab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "intercept                                  2.067840\n",
       "poly(horsepower, degree=2, raw=True)[0]    0.033019\n",
       "poly(horsepower, degree=2, raw=True)[1]    0.000120\n",
       "dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quad_model = MS([poly('horsepower', 2, raw=True)])\n",
    "quad_func = partial(boot_OLS,\n",
    "                    quad_model,\n",
    "                    'mpg')\n",
    "boot_SE(quad_func, Auto, B=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9be038-2582-4e02-81f7-e420b579127c",
   "metadata": {},
   "source": [
    "We compare the results to the standard errors computed using `sm.OLS()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a94305c4-bfe3-4d56-b7d3-7c00a3d9b3e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "intercept                                  1.800\n",
       "poly(horsepower, degree=2, raw=True)[0]    0.031\n",
       "poly(horsepower, degree=2, raw=True)[1]    0.000\n",
       "Name: std err, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M = sm.OLS(Auto['mpg'],\n",
    "           quad_model.fit_transform(Auto))\n",
    "summarize(M.fit())['std err']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278c6959-9713-480a-bbe7-b58502a9d49f",
   "metadata": {},
   "source": [
    "*These exercises were adapted from :* James, Gareth, et al. An Introduction to Statistical Learning: with Applications in Python, Springer, 2023."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
