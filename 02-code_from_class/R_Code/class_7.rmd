---
title: "class_7"
output: html_document
date: "2023-03-06"
---

In class, we reviewed classification and regression trees; this code also includes random forests, that we did not review together. It is included here for your interest.

# Classification trees

Let's use the Carseats dataset (used in textbook). It contains information about sales of carseats in 400 different stores.

```{r}

#load our libraries
library(tree)
library(ISLR2)

#attach our data
attach(Carseats) #as always, explore with ?, str, etc.

#take a look at sales (will be our response variable -- but it's continuous!)
str(Carseats$Sales)
range(Carseats$Sales)

#let's change our numeric, continuous variable to qualitative/binary
#we are doing this to allow us to create a classification tree
High <- factor(ifelse(Sales <= 8, 'No', 'Yes'))

#let's add High back to our dataset
Carseats <- data.frame(Carseats, High)

```

Now, we are ready to fit our tree. To get a feel for the code, we will start by working with the full dataset (note: when we do things more properly, we'll fit to our training set).

```{r}

#let's make a decision tree
#note that we remove the Sales variable because it's redundant with High 
tree.carseats <- tree(High ~ . - Sales, Carseats) 

#plot our tree (remember, run the 'plot' and 'text' in conjunction)
plot(tree.carseats)
text(tree.carseats, pretty=0, cex=.3) #change cex to change size of text

#summary
summary(tree.carseats) #has info about misclassification

#we can also see exact values (perhaps more clearly)
tree.carseats

```
Not discussed in class: R has some special libraries for plotting decision trees, which can make things a bit more visually interpretable.
```{r}
#library
library(rpart)
library(rpart.plot)

#the plotting function requires us to fit the model with its own functions, so let's do that again 
#note that 
tree.carseats_pretty = rpart(High ~ . - Sales, data=Carseats, method = "class")

#better visualization
rpart.plot(tree.carseats_pretty, type=3, clip.right.labs = F) #can remove arguments
```

Now, let's do things a bit more properly. To better estimate error, we need a training and a testing set - because we estimate error in the test set.

```{r}
set.seed(123)
train <- sample(x=nrow(Carseats), size=nrow(Carseats)/2)
test <- Carseats[-train,]
High.test <- High[-train]
```

Now, we can go ahead and use our well-known `predict` function, to predict responses for the test. set.

```{r}
#fit model on training
tree.carseats <- tree(High ~ . - Sales, Carseats, subset=train)

#prediction
tree.pred <- predict(tree.carseats, test, type='class')

#confusion matrix
table(tree.pred, High.test)

#compute classification error -- remember bedmas!
(35 + 13) / 200

```

Great! Now we have an estimate of error in the test set. However, it may still be the case that our tree is too complex. Reducing complexity may improve our test error. The function `cv.tree()` performs crOss-validation to find the best level of tree complexity.

```{r}

set.seed(123)

#perform cross validation
#prune.miscall indicates that we want to use the classification error rate to guide the pruning process (cf. deviance)
cv.carseats <- cv.tree(tree.carseats, FUN=prune.misclass)

#review cv
cv.carseats 

#let's look at a plot - want to find minimal error
plot(cv.carseats$size, cv.carseats$dev, type='b')

#we can also review minimal error out of data, etc
sort(cv.carseats$dev)
```

It looks like 13 and 20 have the lowest error -- let's fit those models and compare error to each other, and the initial tree (before pruning).

```{r}
#let's fit on 13
prune.carseats_13 <- prune.misclass(tree.carseats, best=13)
plot(prune.carseats_13)
text(prune.carseats_13, pretty=0, cex=.4)

summary(prune.carseats_13)#0.14

#let's fit on 20
prune.carseats_20 <- prune.misclass(tree.carseats, best=20)
plot(prune.carseats_20)
text(prune.carseats_20, pretty=0, cex=.4)

summary(prune.carseats_20) #0.115

#both are lower than error before pruning, at ~.24

```

# Regression tree

```{r}
#get data
attach(Boston)

#set seed
set.seed(123)

#set up training/testing sets
train <- sample(nrow(Boston), nrow(Boston)/2)
Boston.test <- Boston[-train,]
medv.test <- Boston[-train, 'medv']

#fit our model in the training est
tree.boston <- tree(medv~ ., Boston, subset=train)

#review summary
summary(tree.boston)

#plot using basic plotting function - can also check out rpart.plot
plot(tree.boston)
text(tree.boston, pretty=0)

#cross validation 
cv.boston <- cv.tree(tree.boston)

#let's find best size (number of terminal nodes)
plot(cv.boston$size, cv.boston$dev, type='b')
#in this case, it looks like 7 terminal nodes ideal - don't need to prune

#let's make predictions in testing set
predict <- predict(tree.boston, newdata=Boston.test)

#calculate MSE
mean((predict - medv.test)^2)
```

# Random Forest

```{r}
#load libraries
library(randomForest)

#set seed
set.seed(123)

#fit random forest
#the argument mtry = 6 indicates that 6 of the predictors should be onsidered for each split of the tree -- see slides for why this is important
rf.boston <- randomForest(medv ~., data = Boston, subset = train, mtry = 6, importance = TRUE)

#make predictions in test set
medv.rf <- predict(rf.boston, newdata = Boston.test)

#compute MSE
mean((medv.rf - medv.test)^2)

#we can review variable importance with convience function
importance(rf.boston)

#we can plot variable importance
varImpPlot(rf.boston)
```


