{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab7cab3e-c7ae-433c-bb69-938a296c3acf",
   "metadata": {},
   "source": [
    "# 6.8: Support Vector Machines Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef777465-b96f-4b0a-bd43-7247d3373260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libaries and objects\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import subplots, cm\n",
    "import sklearn.model_selection as skm\n",
    "from ISLP import load_data, confusion_table\n",
    "from sklearn.svm import SVC\n",
    "from ISLP.svm import plot as plot_svm\n",
    "from sklearn.metrics import RocCurveDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88d30b4-7ec6-48d6-86eb-166b3f2fe947",
   "metadata": {},
   "source": [
    "We will use the function `RocCurveDisplay.from_estimator()` to produce several ROC plots, using a shorthand `roc_curve`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128d6368-c778-49c2-abf7-738310dc4df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_curve = RocCurveDisplay.from_estimator # shorthand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54301b4-c61a-45a3-a07b-a22d4acd00bd",
   "metadata": {},
   "source": [
    "### Support Vector Classifier\n",
    "\n",
    "We'll use the `SupportVectorClassifier()` function (abbreviated `SVC()`) from sklearn to fit the support vector classifier for a given value of the parameter `C`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c2df4e-137b-44ae-a420-2735e753bb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether the classes are linearly separable (they are not)\n",
    "rng = np.random.default_rng(1)\n",
    "X = rng.standard_normal((50, 2))\n",
    "y = np.array([-1]*25+[1]*25)\n",
    "X[y==1] += 1\n",
    "fig, ax = subplots(figsize=(8,8))\n",
    "ax.scatter(X[:,0],\n",
    "           X[:,1],\n",
    "           c=y,\n",
    "           cmap=cm.coolwarm);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969593ce-3d9f-41bd-b6ee-f00642d0427f",
   "metadata": {},
   "source": [
    "We can see that the observations from each class cannot be separated by a hyperplane so we can’t use the\n",
    "maximal margin classifier. We will fit a support vector classifier instead. To start let’s put our observations\n",
    "in a data frame with the response coded as a factor.\n",
    "\n",
    "Now we can use the `SVC()` function to fit our classifier. We choose kernal = \"linear\" to specify we want\n",
    "to fit a support vector classifier and set cost = 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1e8455-0f95-4f03-9df0-7d346a1be5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the classifer\n",
    "svm_linear = SVC(C=10, kernel='linear')\n",
    "svm_linear.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f52fdf-dd1f-4567-ae3e-c4b284f7c75a",
   "metadata": {},
   "source": [
    "The support vector classifier with two features can be visualized by plotting values of its *decision function*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f43d0b9-7f2b-4ada-a879-73fee85f0f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = subplots(figsize=(8,8))\n",
    "plot_svm(X,\n",
    "         y,\n",
    "         svm_linear,\n",
    "         ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20d2790-ef23-40c4-a2c6-0f734c74e60b",
   "metadata": {},
   "source": [
    "**Try fitting a support vector classifier with a smaller value for cost. Compare the results\n",
    "including the number of support vectors to the above two classifiers. From this information\n",
    "explain the relationship between the cost parameter and C as we have described in the slides.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a1ab63-c4cd-49d1-8416-cddab55a7c1e",
   "metadata": {},
   "source": [
    "Now we tune the support vector using `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34cad99-2b9a-4ff0-8974-60fc5aba073c",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = skm.KFold(5, \n",
    "                  random_state=0,\n",
    "                  shuffle=True)\n",
    "grid = skm.GridSearchCV(svm_linear,\n",
    "                        {'C':[0.001,0.01,0.1,1,5,10,100]},\n",
    "                        refit=True,\n",
    "                        cv=kfold,\n",
    "                        scoring='accuracy')\n",
    "grid.fit(X, y)\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eacf240-9946-4f95-ae77-8be58f7e951a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cross-validation errors\n",
    "grid.cv_results_[('mean_test_score')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bcb8e8-7ce5-40e8-96b4-b37025769a4c",
   "metadata": {},
   "source": [
    "We see that `C=1` results in the highest cross-validation accuracy of 0.74, though the accuracy is the same for several values of `C`. The classifier `grid.best_estimator_` can be used to predict the class label on a set of test observations. Let’s generate a test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986bc3c6-1e14-4a07-8539-0b3171739cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = rng.standard_normal((20, 2))\n",
    "y_test = np.array([-1]*10+[1]*10)\n",
    "X_test[y_test==1] += 1\n",
    "\n",
    "# Predictions\n",
    "best_ = grid.best_estimator_\n",
    "y_test_hat = best_.predict(X_test)\n",
    "confusion_table(y_test_hat, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6f98ee-784a-434e-a9d4-351d32a272d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ = grid.best_estimator_\n",
    "y_test_hat = best_.predict(X_test)\n",
    "confusion_table(y_test_hat, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e091efe-42ec-4cda-8f3c-2b69dcd054b9",
   "metadata": {},
   "source": [
    "With this value of `C`, 70% of the test observations are correctly classified"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95b652a-a2ca-4bbe-8bc3-b97b137b61ea",
   "metadata": {},
   "source": [
    "### Support Vector Machine\n",
    "We will once again use the SVC() function to fit an SVM with a non-linear kernel. We first generate some data with a\n",
    "non-linear class boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee372d8-09cc-4d36-b315-7d68d6e62b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "X = rng.standard_normal((200, 2))\n",
    "X[:100] += 2\n",
    "X[100:150] -= 2\n",
    "y = np.array([1]*150+[2]*50)\n",
    "\n",
    "# Plot data to check for non-linearity\n",
    "fig, ax = subplots(figsize=(8,8))\n",
    "ax.scatter(X[:,0],\n",
    "           X[:,1],\n",
    "           c=y,\n",
    "           cmap=cm.coolwarm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e92071-e016-45d0-868d-1c4846023f91",
   "metadata": {},
   "source": [
    "The data is randomly split into training and testing groups. We then\n",
    "fit the training data using the `SVC()`  estimator with a\n",
    "radial kernel and $\\gamma=1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76aa110-774d-4d1f-9b51-5368680a881a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splot into training and testing\n",
    "(X_train, \n",
    " X_test,\n",
    " y_train,\n",
    " y_test) = skm.train_test_split(X,\n",
    "                                y,\n",
    "                                test_size=0.5,\n",
    "                                random_state=0)\n",
    "svm_rbf = SVC(kernel=\"rbf\", gamma=1, C=1)\n",
    "svm_rbf.fit(X_train, y_train)\n",
    "\n",
    "# Plot\n",
    "fig, ax = subplots(figsize=(8,8))\n",
    "plot_svm(X_train,\n",
    "         y_train,\n",
    "         svm_rbf,\n",
    "         ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4293fcd5-8522-4843-9ce4-3b11992e3a8d",
   "metadata": {},
   "source": [
    "**Perform cross-validation using `skm.GridSearchCV()` to select the best choice of gamma and cost. Try the\n",
    "ranges cost = c(0.1, 1, 10, 100, 1000) and gamma = c(0.5, 1, 2, 3, 4). Output the best\n",
    "model.**\n",
    "\n",
    "**Using the best model, predict the responses for the test set. What percent of observations were\n",
    "misclassified by this SVM?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f349e8e-9637-4aa8-8ff4-d07405898a46",
   "metadata": {},
   "source": [
    "*These exercises were adapted from :* James, Gareth, et al. An Introduction to Statistical Learning: with Applications in Python, Springer, 2023."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
